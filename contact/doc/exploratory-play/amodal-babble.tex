
\begin{figure}[bt]
\centerline{\includegraphics[height=6cm]{images/amodal-modules}}
\caption {
%
\label{fig:amodal-module-vision}
%
The system for stochastic exploration and amodal stimulation.
Exploration exercises a set of motor groups, moving them
to random locations within their nominal ranges.
Amodal stimulation modulates the state of a motor group
in a rhythmic fashion, and looks for signs of that rhythm 
showing up in a sensor group. If the amodal signal does in
fact get transmitted, a reinforcement signal is sent to the
exploration model, biasing further exploration towards the
current configuration of the motor groups.
%
The specific scenario shown here is for visual hand/arm coordination.
The state of the head and arm are explored, while watching for
an amodal connection between the arm and vision.
%
}
\end{figure}

\section{Stochastic exploration and amodal stimulation}

\label{sect:motor-babble}

We have previously investigated methods for learning certain motor
skills, such as visually-guided reaching.  As part of the CONTACT
project, UGDIST is considering whether there are mechanisms for
learning such skills that are applicable to both speech and
manipulation.
%
This work revisits themes already examined, but under a more general
framework.

Figure~\ref{fig:amodal-module-vision} shows a crude but general system
for learning certain motor-sensor mappings.  We assume that the
learning problem will be posed as follows: given a collection of motor
groups and sensor groups, and a particular feature derived from the
sensed data, find a configuration of the motor groups such that local
changes in that configuration lead to systematic changes in the
derived feature.

More formally, suppose we have motor control variables
$m=(m_1,m_2,...,m_N)$ and sensor variables $s=(s_1,s_2,...,s_n)$.
Suppose we have some chosen feature of interest $f(s)$.  We know that
$s$ is a function of both $m$ and a set of environmental variables
representing all other influences and state, $e=(e_1,e_2,...)$.
We are interested in how $f$ changes with $m$:

\begin{equation}
\nabla f  = \left(\frac{\partial f}{\partial m_1 }, \frac{\partial f}{\partial m_2 }, \dots,  \frac{\partial f}{\partial m_N }  \right)
\end{equation}

We would like to maximize $|\nabla{}f|$.  We estimate this value
directly through measurement by systematically perturbing $m$ and
measuring $f$.
%
In fact, we really are only concerned with making $|\nabla{}f|$ 
non-zero, since it is quite possible that in much of the motor space 
changes in $m$ have completely no impact on $f$.
%
In that case, we can measure the change in a random motor 
direction $\Delta{}m=(\Delta{}m_1,\Delta{}m_2,...,\Delta{}m_N)$.
%
Unless we are unlucky enough to pick a direction orthogonal to 
the gradient, we should see some change in $f$.
%
However, as we are perturbing $m$, other environmental variables
may also have been changing, causing arbitrary effects on $f$.
We can make this less problematic by simply repeating our perturbation
rhythmically and seeing if the behavior on $f$ seems truly
systematic.

When we detect that $f$ appears to be influenced by $m$ in this
sense, we have discovered a configuration of $m$ which could
be useful as a starting point for exploring the local mapping
from motors ($m$) to sensors ($s$) or between sets of motors.
We call this a ``resonance.''
In the abstract, this discovery is similar to the 
infant's ``discovery'' of its hand in its field of view.
Figures~\ref{fig:amodal-module-vision} and \ref{fig:find-arm}.
show an analogous experiment for a robot, where a configuration
of the arm and head that brings the arm into view is discovered.

Since $f$ is computed based on sensor input, we should
expect to occasionally be mislead.  Therefore, rather 
than stopping once we find a good value of $m$, we use
a stochastic search method.  
%
Currently, we maintain a population of possible motor configurations,
initially random.  In each step, we select a configuration randomly,
perturb it (or with a small probability randomize it entirely), and if
it results in an apparent resonance, we copy that configuration to 10\%
of the population.
%
A configuration that consistently results in resonance therefore quickly 
propagates to the entire population.


\begin{figure}[p]
\centerline{\includegraphics[width=\textwidth]{images/find-arm}}
\caption{
%
\label{fig:find-arm}
%
A simulated robot, performing motor exploration on its
head/eyes and left arm.  The head, eyes, and arms move around randomly
within their range of movement.
%
A periodic ``shaking'' movement of the arm is made about its
current position, and the effect of this movement is searched 
for visually.
%
When resonance occurs, the configuration of head/eyes and left arm
is reinforced so that the robot is more likely to return to that
configuration.
%
This is why 
in early snapshots (top) the robot is unlikely to be looking at 
its arm, and in later snapshots (bottom) the robot is looking at
its arm more often than not.
%
}
\end{figure}


We would like to stress that developing a general purpose learning
mechanism of this kind for a robot does not mean be believe the robot
should start off its developmental process as a ``tabula rasa.''  In
fact, there may well be just as much or more information involved in
specifying how to explore (expressing the motor groups to use, their
scales, and what sensory cue to monitor) as there would be in just
giving numerical pose information.
%
The advantage we are looking for is robustness to the details of 
the motor and perceptual system, such that changes in scale or the
ranges of joints do not disrupt everything.
%
We want a way of expressing capabilities such as ``bring the hand in
front of head'' which can be developed operationally, rather than
relying on parameters supplied for a particular instantiation
of a robot body.

As a side benefit, our robot should not be completely stumped if
expected to control its arm seem via a mirror or other distortion.
Any transformation that is spatially smooth should be manageable;
however delays introduced in perception would be as disastrous for our
system as they are for humans.

Of course, the specific method described so far is very crude.  It is
intended only to locate an initial point of contact between motor
spaces or between motor and sensor spaces.  Once that point has been
found, tracking methods would be more efficient for exploring a region
of motor space.



%%Could automate motor space classification examples: gesture
%%recognition, speech recognition

%%Would have to be a lot simpler than offline, human-mediated work of course.


%% \subsection{Towards meaning}

%% The richer the robot's "internal life" is, the more interesting kinds
%% of "meanings" there can be in its world

%% - Continuous, broad, shared, structured experience

%% - Meaning as a process, not a label

%% Off-line data collection, segmentation, labelling, training of
%% classifiers, etc work, but restrict the potential semantic universe of
%% the robot.

%% For Year 3, online behaviors would be useful.

